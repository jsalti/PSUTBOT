# -*- coding: utf-8 -*-
"""scrape_school_info.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TaC7eVptMcQu6epU4w8cDrzxa7wCdvxz
"""

import requests
import json
from bs4 import BeautifulSoup
import os

def scrape_departments(department_container):
    departments = department_container.find_all("a", href=True)

    department_data = []
    for department in departments:
        department_name = department.find("h4").text.strip()
        department_link = department.get("href")

        department_data.append({
            "Department Name": department_name,
            "Link": department_link
        })

    return department_data

def scrape_programs(program_container):
    programs = program_container.find_all("a", href=True)

    program_data = []
    for program in programs:
        program_name = program.find("h4").text.strip()
        program_description = program.find("p").text.strip()
        program_link = program.get("href")

        program_data.append({
            "Program Name": program_name,
            "Description": program_description,
            "Link": program_link
        })

    return program_data

def scrape_school_info(url):
    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.content, "html.parser")

        # Find the container for the department information
        department_container = soup.find("div", class_="tab-pane fade", id="nav-profile")

        # Extract information about departments
        department_data = scrape_departments(department_container)

        # Find the container for the program information
        program_container = soup.find("div", class_="tab-pane fade show active")

        # Extract information about programs
        program_data = scrape_programs(program_container)

        # Combine department and program data into a single dictionary
        combined_data = {
            "Departments": department_data,
            "Programs": program_data
        }

        return combined_data

    else:
        print(f"Failed to retrieve the page for URL: {url}")
        return None

# Example URLs
urls_list = [
    "https://psut.edu.jo/en/school/School_of_Engineering",
    "https://psut.edu.jo/en/school/King_business_technology",
    "https://psut.edu.jo/en/school/school_of_computing_sciences#nav-home"
]

# Combine data for all URLs
all_data = {}
for url in urls_list:
    data_for_url = scrape_school_info(url)
    if data_for_url:
        all_data[url] = data_for_url

# Specify the output directory
output_directory_school = '/tmp/output_school'

# Create the output directory if it doesn't exist
os.makedirs(output_directory_school, exist_ok=True)

# Save all the combined data to a single JSON file
json_file_path_all_data = os.path.join(output_directory_school, 'all_combined_data.json')
with open(json_file_path_all_data, 'w', encoding='utf-8') as json_file:
    json.dump(all_data, json_file, ensure_ascii=False, indent=2)

print(f"Data for all URLs saved to {json_file_path_all_data}")

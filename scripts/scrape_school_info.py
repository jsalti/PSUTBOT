# -*- coding: utf-8 -*-
"""scrape_school_info.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PL2U_LBEYJhxlKXo_d6chOO-BSAkFhz8
"""

import os
import requests
import fitz  # PyMuPDF
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import chromedriver_autoinstaller
import time
import json

import os
import requests
from bs4 import BeautifulSoup
import json

def scrape_school_info(urls):
    all_departments = []
    all_programs = []

    for url in urls:
        # Send a GET request to the URL
        response = requests.get(url)

        # Check if the request was successful
        if response.status_code == 200:
            try:
                # Parse the HTML content using BeautifulSoup
                soup = BeautifulSoup(response.text, "html.parser")

                # Find the container for the department information
                department_container = soup.find("div", class_="tab-pane fade", id="nav-profile")

                # Check if the container is found
                if department_container:
                    # Extract information about departments
                    departments = department_container.find_all("a", href=True)

                    department_data = []
                    for department in departments:
                        # Check if the <h4> tag is found
                        h4_tag = department.find("h4")
                        if h4_tag:
                            department_name = h4_tag.text.strip()
                            department_link = department.get("href")

                            department_data.append({
                                "Department Name": department_name,
                                "Link": department_link
                            })
                        else:
                            print(f"No <h4> tag found for department in URL: {url}")

                    all_departments.append(department_data)

            except Exception as e:
                print(f"Error processing URL {url}: {e}")
        else:
            print(f"Department container not found for URL: {url}")

    return all_departments, all_programs

# Rest of the code remains the same


# Rest of the code remains the same


# Example URLs
urls_list = [
    "https://psut.edu.jo/en/school/School_of_Engineering",
    "https://psut.edu.jo/en/school/King_business_technology",
    "https://psut.edu.jo/en/school/school_of_computing_sciences#nav-home"
]

# Call the function with the list of URLs
result_departments, result_programs = scrape_school_info(urls_list)

# Specify an alternative writable directory (e.g., '/tmp/output_school')
output_directory_school = '/tmp/output_school'

# Create the output directory if it doesn't exist
os.makedirs(output_directory_school, exist_ok=True)

# Save the extracted data to JSON files in the specified directory
for i, departments_data in enumerate(result_departments):
    json_file_path_departments = os.path.join(output_directory_school, f'school_departments_{i}.json')
    with open(json_file_path_departments, 'w', encoding='utf-8') as json_file_departments:
        json.dump(departments_data, json_file_departments, ensure_ascii=False, indent=2)
    print(f'Data saved to: {json_file_path_departments}')

for i, programs_data in enumerate(result_programs):
    json_file_path_programs = os.path.join(output_directory_school, f'school_programs_{i}.json')
    with open(json_file_path_programs, 'w', encoding='utf-8') as json_file_programs:
        json.dump(programs_data, json_file_programs, ensure_ascii=False, indent=2)
    print(f'Data saved to: {json_file_path_programs}')